{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b740bf",
   "metadata": {},
   "source": [
    "# Problem set 3 -- Neural Amortized Inference\n",
    "\n",
    "Save the completed notebook as a pdf file and submit the pdf file to Canvas. Follow examples in the `Data-Driven Proposals in Gen` notebook (Section 5) covered in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc3db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using Gen\n",
    "using Plots\n",
    "using DelimitedFiles\n",
    "using JLD2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436fa85d",
   "metadata": {},
   "source": [
    "## Perception in a rectangle world\n",
    "\n",
    "In this pset, you will develop a perception system that operates in a two-dimensional grayscale world where all objects are axis-aligned rectangular frames (i.e., unfilled rectangles) and there is just one such object in a given scene. An example scene in this world is illustrated below. Given such an input, the perception system should provide a posterior over where the object is, its constrast and size.\n",
    "\n",
    "<img src=\"./images/examples.png\"  width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b04eb0",
   "metadata": {},
   "source": [
    "### Q 1A [2.5 pts]\n",
    "\n",
    "Your first task is to write a generative model of this process. You will do this in the generative function `two_d_world`, below.\n",
    "\n",
    "Here are the basic assumptions your generative model should reflect.\n",
    "\n",
    "* Assume that the world size is 10x10 pixels.\n",
    "* Assume that there is one object in each scene in this world. \n",
    "* An object's position, in particular its bottom-left corner, can be anywhere in the world. So in a lot of the scenes, the object will only be partially visible. \n",
    "* Each dimension of the objects in this world (width and height) follow a uniform distribution between 3 to 7 pixels. Notice that a rectangle cannot have a negative dimension.\n",
    "* An object's overall brightness can vary between 0.1 and 1 with a uniform distribution, where the background brigthness is set to 0. \n",
    "* Finally, assume that the observations are corrupted by some small Gaussian noise (mean, std = 0.05), i.e., adding a Gaussian noise to the brightness of each pixel.\n",
    "\n",
    "\n",
    "To make our variational approximation less of a pain, we recommend setting up each of your priors to be uniform distributions `[0, 1]`, then scaling them before \"rendering\" your object.\n",
    "</details>\n",
    "\n",
    "We provide examples for two of the relevant random variables &ndash; the y-coordinate of the south-west (bottom-left) of the object and the height of the object.\n",
    "\n",
    "```julia\n",
    "# draw where the object's y coordinate will be\n",
    "SW_row ~ uniform(0, 1)\n",
    "# draw the height of the object\n",
    "h ~ uniform(0, 1)\n",
    "\n",
    "# scale the y-coordinate so that it is an integer (we will use this to index into a Matrix of 10x10, and lies between 1 and 10).\n",
    "scaled_SW_row = ceil(Int64, SW_row * 10)\n",
    "# scale the height so that it lies between 3 and 7 and is an integer\n",
    "scaled_h = round(Int64, h * 4 + 3)\n",
    "\n",
    "```\n",
    "\n",
    "All random variables: x, y coordinate of the left corner, height, width, overall brightness of the object, brightness of each pixel (after considering whether it belongs to the object as well as the gaussian noise). Hint: brightness of each pixel can be sampled from `normal(mean brightness conditioned on whether it is part of the object or the background, 0.05)`. \n",
    "\n",
    "In total, there are 100 (all pixels' brightness) + 5 (location, size, object brightness) random variables.\n",
    "\n",
    "The generative function should return a 10x10 matrix as the rendered image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COLS = 10\n",
    "N_ROWS = 10\n",
    "\n",
    "@gen function two_d_world()\n",
    "    # sample unscaled y coordinate (bottom-left corner)\n",
    "    SW_row ~ uniform(0, 1)\n",
    "    \n",
    "    # sample unscaled x coordinate (bottom-left corner)\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "    \n",
    "    # sample unscaled height\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "    \n",
    "    # sample unscaled width\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "    \n",
    "    # rescale x, y, height, and width to integers\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "    \n",
    "    # sample overall brightness for the object\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "    \n",
    "    # create a blank image\n",
    "    image = Matrix{Float64}(undef, N_ROWS, N_COLS) \n",
    "    \n",
    "    # sample brightness for each pixel with noise\n",
    "    for row_id in 1:N_ROWS\n",
    "        for col_id in 1:N_COLS\n",
    "            # compute the mean_brightness for this pixel\n",
    "            # check whether it is part of the object or the background\n",
    "            # your code here\n",
    "            throw(Exception(\"Not Implemented.\"))\n",
    "            \n",
    "            image[row_id, col_id] = {:image => row_id => col_id => :brightness} ~ normal(mean_brightness, 0.05)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # return image\n",
    "    image\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a026e",
   "metadata": {},
   "source": [
    "Below is a function to visualize a given draw from your generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40bc06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function visualize(input::Matrix{<:Real})\n",
    "    heatmap(input, clim=(0,1), thickness_scaling=3.5, size=(1600, 1300), aspect=:equal)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d2cfd",
   "metadata": {},
   "source": [
    "Draw a sample from your generative model and visualize it (using the `visualize` function above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d840463",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(42)\n",
    "visualize(two_d_world())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f2e81c",
   "metadata": {},
   "source": [
    "### Q 1B [2 pts]\n",
    "Now implement an amortized variational approximation of this generative model, parametrized with a deep neural network conditioning this approximation to input observations. You will do this in the generative function `neural_amortized_inference`, below.\n",
    "\n",
    "Assume that the neural network takes as input a vector &ndash; so, the observations should be flattened to vectors (from 2D matrices). Your network architecture should be rather simple: one hidden layer and one output layer. The hidden layer should be activated with a `tanh` non-linearity (provided in the code block below).\n",
    "\n",
    "The output layer should consist of all of the variational family parameters. \n",
    "\n",
    "As for your variational approximation, for a random variable `x ~ uniform(0,1)` in your generative model (`two_d_world`), a reasonable choice would be `x ~ beta(shape, scale)`. Your neural network would be outputting the parameters of the beta, but you'd need to be careful to ensure that these parameters are possitive. E.g., you can use exp() as the activation function for the output layer: `exp.(x)`. \n",
    "\n",
    "HINT: There are 5 (location, size, object brightness) random variables we want to infer based on an observed image. So there should be 10 parameters from the output layer of the neural network (2 parameters for the beta distribution for each random variable).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992fb26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ïƒ(x) = tanh.(x)\n",
    "\n",
    "@gen function neural_amortized_inference(input::Vector{Float64})\n",
    "    # write out the architecture of your model using `@params` to define weights and biases\n",
    "    # and then the forward pass (the matrix multiplications, non-linearity etc.)\n",
    "    # there should be 4 parameters in total; 2 for each layer\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "\n",
    "    # non-linear hidden layer\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\")) \n",
    "    \n",
    "    # output layer\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "\n",
    "    # collect the variational approximation parameters from the output layer\n",
    "    # then use thse parameters to sample the latent variables\n",
    "    \n",
    "    # here is an example -- sampling SW_row (the y coordiate of the bottom-left corner)\n",
    "    SW_row_shape = output_layer[1]\n",
    "    SW_row_scale = output_layer[2]\n",
    "    SW_row ~ beta(SW_row_shape, SW_row_scale)\n",
    "    \n",
    "    # sample the remaining four variables based on the parameters from the output layer\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "    \n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfdfe0d",
   "metadata": {},
   "source": [
    "### Q 1C [1 pts]\n",
    "\n",
    "Next create a data generator function, called `data_generator`. Notice that this function takes no arguments. In each call, it will simulate the generative model of our world once. This will yield a pair of input and output for training the neural network based estimator `neural_amortized_inference`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "function data_generator()\n",
    "    tr = Gen.simulate(two_d_world, ())\n",
    "\n",
    "    # record the \"observations\" (inputs to the NN model, i.e., brightness of each pixel)\n",
    "    obs_matrix = Matrix{Float64}(undef, N_ROWS, N_COLS)\n",
    "    \n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "    \n",
    "    obs = vec(obs_matrix)\n",
    "    \n",
    "    # record the random choices of the 5 latent variables (outputs of the NN model)\n",
    "    constraints = Gen.choicemap()\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "    \n",
    "    return ((obs,), constraints)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6dd119",
   "metadata": {},
   "source": [
    "### Q 1D [1.5 pts]\n",
    "\n",
    "Initialize the `params` in the `neural_amortized_inference`. You will have to pay attention to your dimensions.\n",
    "\n",
    "Choose the dimensionality of the hidden layer to be 200. Use the `init_weight` function (provided in the code block below) to initialize your weight matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f346a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(42)\n",
    "# a function for randomly initializing the weight matrices\n",
    "init_weight(shape...) = (1. / sqrt(shape[2])) * randn(shape...)\n",
    "\n",
    "# input and output dimensions of the network\n",
    "# your code here\n",
    "throw(Exception(\"Not Implemented.\"))\n",
    "\n",
    "# choose the number of units each layer of the network\n",
    "# your code here\n",
    "\n",
    "# create and initialize W1 and W2\n",
    "# your code here\n",
    "throw(Exception(\"Not Implemented.\"))\n",
    "\n",
    "# now initialize the params of the data-driven proposal function\n",
    "init_param!(neural_amortized_inference, :W1, init_W1)\n",
    "init_param!(neural_amortized_inference, :b1, zeros(num_hidden))\n",
    "init_param!(neural_amortized_inference, :W2, init_W2)\n",
    "init_param!(neural_amortized_inference, :b2, zeros(output_dim));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c1e710",
   "metadata": {},
   "source": [
    "### Q 1E [1 pt]\n",
    "\n",
    "Create an optimizer for updating the weights using `Gen.FixedStepGradientDescent` with a learning rate of `1e-5`. \n",
    "\n",
    "Train your amortized estimator using this optimizer using `Gen.train!`. \n",
    "\n",
    "Use the following arguments for the `train!` function:\n",
    "```\n",
    "num_epoch=200\n",
    "epoch_size=1000\n",
    "num_minibatch=100\n",
    "minibatch_size=10\n",
    "evaluation_size=100\n",
    "verbose=true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e938e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a gradient-based optimizer and train!\n",
    "# your code here\n",
    "throw(Exception(\"Not Implemented.\"))\n",
    "\n",
    "# save trained parameters\n",
    "let data = Dict()\n",
    "    for name in [:W1, :b1, :W2, :b2]\n",
    "        data[(:param, name)] = Gen.get_param(neural_amortized_inference, name)\n",
    "    end\n",
    "    save(\"neural_amortized_inference_trained.jld2\", \"data\", data)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170feab0",
   "metadata": {},
   "source": [
    "The following code loads a test observation and visualizes it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aabbaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_matrix = readdlm(\"test-scene.txt\")\n",
    "obs = vec(obs_matrix)\n",
    "obs = convert(Vector{Float64}, obs)\n",
    "p1 = visualize(obs_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69fcf89",
   "metadata": {},
   "source": [
    "### Q 1F [2 pts] <a id=\"q-1f\"></a>\n",
    "\n",
    "Conditioned on this observation, run importance sampling with or without `neural_amortized_inference` as the data-driven proposal. Compare the average log probabilities of the two methods. The importance sampling with `neural_amortized_inference` proposal should have significantly better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f15f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained parameters\n",
    "let data = JLD2.load(\"neural_amortized_inference_trained.jld2\", \"data\")\n",
    "    for name in [:W1, :b1, :W2, :b2]\n",
    "        Gen.init_param!(neural_amortized_inference, name, data[(:param, name)])\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be6e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "function logmeanexp(scores)\n",
    "    logsumexp(scores) - log(length(scores))\n",
    "end;\n",
    "\n",
    "# Make constraints based on the observed image (obs_matrix)\n",
    "contraints = Gen.choicemap()\n",
    "# your code here\n",
    "throw(Exception(\"Not Implemented.\"))\n",
    "\n",
    "# run importance sampling *without* the neural amortized inference, amount of compute = 100, repeat 10 times\n",
    "# your code here\n",
    "throw(Exception(\"Not Implemented.\"))\n",
    "        \n",
    "scores = [get_score(t) for t in traces]\n",
    "\n",
    "println(logmeanexp(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run importance sampling *with* the neural amortized inference, amount of compute = 100, repeat 10 times\n",
    "# your code here\n",
    "throw(Exception(\"Not Implemented.\"))\n",
    "\n",
    "scores = [get_score(t) for t in traces]\n",
    "\n",
    "println(logmeanexp(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
